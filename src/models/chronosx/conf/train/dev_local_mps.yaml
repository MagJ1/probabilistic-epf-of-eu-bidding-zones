# conf/train/default.yaml

# per-device batch sizes
batch_size: 2           # maps to per_device_train_batch_size
eval_batch_size: 2       # maps to per_device_eval_batch_size

# optimizer & LR scheduling
learning_rate: 1e-3
scheduler: linear        # e.g. linear | cosine | constant | cosine_with_restarts
warmup_ratio: 0.05
optim: adamw_torch       # on cuda adamw_torch_fused

# logging / checkpointing / schedule
log_steps: 1
enable_tensorboard: true
save_steps: 1000000
eval_steps: 1000000
max_steps: 20
save_total_limit: 3
load_best_model_at_end: true
enable_progress_bar: false

# gradient accumulation & eval accumulation
grad_accum: 2            # maps to gradient_accumulation_steps
eval_accum_steps: 4

early_stopping_enabled: true
early_stopping_patience: 30     # HF counts
early_stopping_threshold: 0.0

# dataloader workers
num_workers: 0               

